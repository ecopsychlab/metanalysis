---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# metanalysis

<!-- badges: start -->
[![GitHub issues](https://img.shields.io/github/issues/ecopsychlab/metanalysis)](https://github.com/ecopsychlab/metanalysis/issues)
[![GitHub pulls](https://img.shields.io/github/issues-pr/ecopsychlab/metanalysis)](https://github.com/ecopsychlab/metanalysis/pulls)
[![R BiocCheck](https://github.com/ecopsychlab/metanalysis/actions/workflows/test.yaml/badge.svg)](https://github.com/ecopsychlab/metanalysis/actions/workflows/test.yaml)
<!-- badges: end -->
The goal of metanalysis is to provide a convenient interface to manage and 
analyse multiple data sets. 

## Installation

You can install the development version of metanalysis from 
[GitHub](https://github.com/) with:

``` r
# install.packages("remotes")
remotes::install_github("ecopsychlab/metanalysis")
```

## Imaginary Usecase

This is a basic example which shows you how to solve a common problem:

```{r examplea}
library(metanalysis)
# We have a list of tables
x <- split.data.frame(mtcars, rep(LETTERS[seq_len(4)], each = 8))
x

```

```{r exampleb}
# Let's first prepare a nested folder structure to keep our tables: 
create_forest_study(x, "demo_folder")

# Confirm that the files are now written: 
list.files("demo_folder", full.names = TRUE, recursive = TRUE)
```

forest_study object could be the main user-oriented interface, for instance to get
summary statistics across data sets. 

```{r meta-studya}

x <- forest_study("demo_folder")
x

```

Maybe some information could be collected like this: 

```{r meta-studyb}
x@study_overview

```

```{r examplec}
# We can treat our folder as a data base and open it using the arrow library: 
library(arrow)

# we can wrap arrow::open_dataset by calling it like this: 

y <- arrow::open_dataset(x@path)
y

# I should ensure this doesn't mess with arrow performance. 

```

Ongoing: 

- Take your datasets in whatever format they are and organise them into an arrow
  compatible dataset folder structure. 
  - XSummarizedExperiments could be decomposed to assay, row and col .parquets. 
    - Need function to read them back from parquet component parts
  - Should have functions to transform common bio formats to parquet. Maybe 
    funnel through mia io? Need to look into this to decide.

- harmonize metadata - see `schema` documentation of arrow

- Produce some sort of overarching metadata file in the study folder. 

Later: 
- define analyses - see `arrow::register_scalar_function()` - looks really nice,
  but almost certainly would require some nice wrapping for our target audience.
  
- Some way to conveniently collect and compare across studies/interactions. 
  meta-meta-data??

- A function to make the totality of the analysis into a github repo, likely 
using R/Qmd

